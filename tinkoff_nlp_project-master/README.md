# Tinkoff NLP Project

## Seq2Seq генеративная болталка

### 1. Предобработка данных: (Поляков, Жолковский)

  ##### Сделано до 14 апреля:
  - Добавление PAD, EOS, SOS. Создание словаря слов.
  - Отбор слов по количеству. Выбор максимальной длины предложения.
  - Очистка текста.
  - Отбор наиболее полезных примеров.
  - Изначально был 21 миллион примеров, из них полезных после чистки остается примерно 4 миллиона.
  - Сейчас учим модель на 100-600 тысячах примеров случайно выбранных примерах, потому что получается лучше.

  ##### В планах до 20 апреля:
  - Более качественный отбор примеров.
  - Более качественная предобработка.
  - Сделать грамотный shuffle.
  - Добавить учет запятых.

### 2. Создание и тестирование архитектуры проекта (Поляков, Жолковский)

  ##### Сделано до 14 апреля:
  - Seq2Seq модель с GRU.
  - Attention механизм по статье "Effective Approaches to Attention-based Neural Machine Translation".
  - Далее тестируем с Attention. Пока получаем плохой результат, тестируем дальше.
  - Подготовили данные для загрузки в Телеграмм бота. (Пока храним в pickle)

  ##### В планах до 20 апреля:
  - Подобрать параметры, улучшить результат.
  - Докрутить Transformer. (Дает хуже результат, не смогли добиться лучше качества, оставляем итоговую модель на seq2seq с attention)
  http://nlp.seas.harvard.edu/2018/04/03/attention.html

### 3. Реализация Telegram бота (Гатауллин)

  ##### Сделано до 14 апреля:
  - Подготовлен шаблон для сервера с моделью
  - Реализован бот с несколькими стандартными ответами, модель пока не прикручена.
  - @Pikatalk_Bot

  ##### В планах до 20 апреля:
  - Загрузить модель и файлы через Google API. (Сделано)
  - Поменять модель на более качественную. (Сделано)


### 4. Оптимизация кода и моделей (Семенова)

  ##### Сделано до 14 апреля:
  - Поиск плохих примеров и дубликатов.
  - Придумано, как ускорить apply на 21 миллионах данных pandas. (Применение swifter)
  - Рассмотрены разные подмножества примеров для обучения.
  - Протестистировано обучение моделей на устойчивость.
  - Тестирование моделей с разным shuffle.

  ##### В планах до 20 апреля:
  - Подобрать оптимальное подмножество для последнего обучения.
  - Разобраться с проблемой переполнения GPU при обучении на всех данных. (Сделано)

### Продвижение по проекту:
  - Реализация моделей: 60-70%
  - Реализация бота: 100%
  
### Пояснения по репозиторию:
  ### Ветка master
   - Ноутбуки с обучением и тестированием модели, разбивка ноутбука на файлы.
   - По следующим ссылкам можно скачать файлы обученной модели encoder, decoder, vocabulary, pairs:
      Они находятся в файле Files.
  ### Ветка telegram_bot
   - Файлы для создания бота
  
### Пояснения по запуску:
  Сейчас есть две немного разные модели. Одна - в юпитер ноутбуке, вторая - запакована в tar и запускается через evaluate.py
  Разница в удалении дубликатов только и другом shuffle. В Ноутбуке получился более положительный бот, от него часто можно услышать, что он Вас любит.Кажется, что модель в ноутбуке лучше отвечает, но это не точно.
### 1.
- Ноутбук Seq2Seq_Rus предназначен для запуска и обучения заново, но в конце можно просто протестировать, скачав файлы
- Ноутбук Seq2Seq_test предназначен для тестирования. Скачав все файлы, можно потестировать бота.
- К боту пока не прикручена модель, но сам бот рабочий и лежит в ветке telegram_bot.
- (Нужно скачать файлы из Files)
### 2.
- Модель также можно запустить через evaluate.py
- Для этого нужно сделать следующее: в терминале сделать
- git clone https://github.com/zhozheka/tinkoff_nlp_project
- Затем wget https://www.dropbox.com/s/69d6bd7ejjhbp4b/4000_checkpoint-2.tar?dl=0
- И после этого python evaluate.py -model ./path_to_model
- Так можно общаться с ботом в командной строке.
(Нужно проделать только эту последовательность действий)

### 3.
- Бота можно запустить в telegram. @Pikatalk_Bot


